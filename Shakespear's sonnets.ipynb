{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sonnets",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOHEV1kiRrLwqTAjUzvybOT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShivanshuPurohit/Practice-models/blob/master/Shakespear's%20sonnets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZB5nyO2uGeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j4Kl5DbzS4R",
        "colab_type": "code",
        "outputId": "71de151e-20ab-4107-9204-aaea448928cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "def load_data(maxlen=25, step=3):\n",
        "    '''\n",
        "    Load in training text and vectorize it into 3D tensor\n",
        "    Args:\n",
        "        maxlen: Maximum length of sequence\n",
        "        step: Step size for sampling new sequence\n",
        "    Returns:\n",
        "        Numpy tensor of shape (samples, seq_len, num_chars)\n",
        "    '''\n",
        "    with open('sonnets.txt', 'r') as f:\n",
        "        data = f.read().lower()\n",
        "\n",
        "    sentences = []\n",
        "    targets = []\n",
        "    # Loop through sonnets and create sequences and associated targets\n",
        "    for i in range(0, len(data) - maxlen, step):\n",
        "        sentences.append(data[i:i + maxlen])\n",
        "        targets.append(data[maxlen + i])\n",
        "    # Grab all unique characters in corpus\n",
        "    chars = sorted(list(set(data)))\n",
        "\n",
        "    # Dictionary mapping unique character to integer indices\n",
        "    char_indices = dict((char, chars.index(char)) for char in chars)\n",
        "    return data, char_indices, chars\n",
        "\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    '''\n",
        "    Reweight the predicted probabilities and draw sample from\n",
        "    newly created probability distribution\n",
        "    Args:\n",
        "        preds: Numpy array of character probabilities\n",
        "        temperature: Float representing randomness of reweighting probabilities\n",
        "                     Higher temp, more randomn. Lower temp, more deterministic\n",
        "    Returns:\n",
        "        Index of largest probability from reweighted probabilities\n",
        "    '''\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def generate_sonnet():\n",
        "    '''\n",
        "    Choose a random seed text and print 600 predicted characters to console\n",
        "    Args:\n",
        "        None\n",
        "    Returns:\n",
        "        600 characters printed to console\n",
        "    '''\n",
        "    # Max length of each sequence. Same as in Jupyter notebook\n",
        "    maxlen = 40\n",
        "    # Sample new sequence every step characters\n",
        "    step = 3\n",
        "    data, char_indices, chars = load_data(maxlen, step)\n",
        "\n",
        "    # Load model trained in Jupyter notebook\n",
        "    model = load_model('shakespeare_sonnet_model.h5')\n",
        "    # Choose random seed text\n",
        "    start_idx = np.random.randint(0, len(data) - maxlen - 1)\n",
        "    new_sonnet = data[start_idx:start_idx + maxlen]\n",
        "    sys.stdout.write(new_sonnet)\n",
        "    for i in range(600):\n",
        "        # Vectorize generated text\n",
        "        sampled = np.zeros((1, maxlen, len(chars)))\n",
        "        for j, char in enumerate(new_sonnet):\n",
        "            sampled[0, j, char_indices[char]] = 1.\n",
        "\n",
        "        # Predict next character\n",
        "        preds = model.predict(sampled, verbose=0)[0]\n",
        "        pred_idx = sample(preds, temperature=0.5)\n",
        "        next_char = chars[pred_idx]\n",
        "\n",
        "        # Append predicted character to seed text\n",
        "        new_sonnet += next_char\n",
        "        new_sonnet = new_sonnet[1:]\n",
        "\n",
        "        # Print to console\n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    ''' Run main program '''\n",
        "    generate_sonnet()\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is lust in action: and till action, lust the brave the proughs wall the with farteres de my so now\n",
            "\n",
            "and stan the will the sire of love me bear,\n",
            "the are thy forle the ans thie beast steet.\n",
            "the ear the frow in thee the with my be to be.\n",
            "\n",
            "and in the strof have to but show the rears,\n",
            "and sting con love wher his thue word all hen my leave,\n",
            "your mare the sweet in where the live bear the prave,\n",
            "when when is his thing of the worth to shat the rear mare\n",
            "the of her as thou shath this what hat me no thee war sweet hade reass to ey,\n",
            "and wilt thou sen be our that his spire steel,\n",
            "and my more the wipl the will with and steet stire,\n",
            "and stare mare\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsT_M1WxzZbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHAORGSczp2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "np.random.seed(21)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7pQyq3hz4wu",
        "colab_type": "code",
        "outputId": "aae1a89b-6f8c-400d-c54f-46cd2a2b12a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "with open('sonnets.txt','r') as f:\n",
        "  data = f.read().lower()\n",
        "print('Corpus length : %d characters' %len(data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus length : 94651 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeA3zzJJ0UF-",
        "colab_type": "code",
        "outputId": "24e14100-e618-4580-88b3-c993fb85427c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "#visualize character length\n",
        "sonnets = data.split('\\n\\n')\n",
        "sonnet_lens = [len(sonnet) for sonnet in sonnets]\n",
        "print('Average sonnet length: %.2f characters' % np.mean(sonnet_lens))\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar([i for i in range(1,len(sonnets)+1)],sonnet_lens)\n",
        "plt.title('Number of Characters per sonnet')\n",
        "plt.ylabel('no. of Characters')\n",
        "plt.xlabel('Sonnets')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average sonnet length: 612.63 characters\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3sAAAJcCAYAAABAE73ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUuA1dlh1QhL",
        "colab_type": "code",
        "outputId": "8be34212-7970-471a-d6ba-32628c6047a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#max length of each sentence\n",
        "maxlen = 40\n",
        "\n",
        "#sample new sequence ever step characters\n",
        "step = 3\n",
        "\n",
        "sentences=[]\n",
        "targets=[]\n",
        "\n",
        "#loop through sonnets and create new sequences and targets\n",
        "for i in range(0,len(data)-maxlen,step):\n",
        "  sentences.append(data[i:i+maxlen])\n",
        "  targets.append(data[maxlen+i])\n",
        "\n",
        "print(\"number of sequences: \",len(sentences))\n",
        "\n",
        "#grab all unique characters in corpus\n",
        "chars = sorted(list(set(data)))\n",
        "print(\"no. of unique characters: \",len(chars))\n",
        "\n",
        "#dictionary mapping unique characters to integer iindices\n",
        "char_indices = dict((char, chars.index(char)) for char in chars)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of sequences:  31537\n",
            "no. of unique characters:  38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PxwvxNR3Z39",
        "colab_type": "code",
        "outputId": "232483e6-8a98-4a58-b200-c0180823bc66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#vectorize sequences and targets\n",
        "x= np.zeros((len(sentences),maxlen,len(chars)),dtype=np.bool)\n",
        "y= np.zeros((len(sentences),len(chars)),dtype=np.bool)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for j, char in enumerate(sentence):\n",
        "    x[i,j,char_indices[char]]=1\n",
        "    y[i,char_indices[targets[i]]]=1\n",
        "\n",
        "print(\"Size of training sequences:\", x.shape)\n",
        "print(\"Size of training targets:\", y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training sequences: (31537, 40, 38)\n",
            "Size of training targets: (31537, 38)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlwmF2w84smD",
        "colab_type": "code",
        "outputId": "824c8346-c701-4973-f76a-d1c73adbd448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256,return_sequences=True,input_shape=(maxlen,len(chars))))\n",
        "model.add(LSTM(64,input_shape=(maxlen,len(chars))))\n",
        "model.add(Dense(len(chars),activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_4 (LSTM)                (None, 40, 256)           302080    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 64)                82176     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 38)                2470      \n",
            "=================================================================\n",
            "Total params: 386,726\n",
            "Trainable params: 386,726\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kfskslwe5dfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD\n",
        "optimizer = SGD(lr=0.01,momentum =0.9,nesterov=True)\n",
        "model.compile(optimizer=optimizer,loss='categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3eay97Y6F6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def samples(preds,temperature=1.0):\n",
        "  preds=np.asarray(preds).astype('float64')\n",
        "  preds=np.log(preds)/temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds=exp_preds/np.sum(exp_preds)\n",
        "  probas=np.random.multinomial(1,preds,1)\n",
        "  return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbyvfvYq67xU",
        "colab_type": "code",
        "outputId": "304315a4-f165-422a-9e78-22237a1610b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 40\n",
        "\n",
        "loss = []  # Custom history list to save model's loss\n",
        "\n",
        "# Create directory to store generated text\n",
        "base_dir = 'generated_text'\n",
        "if not os.path.isdir(base_dir):\n",
        "    os.mkdir(base_dir)\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    print(\"Epoch\", epoch)\n",
        "    # Fit model for 1 epoch then generate text given a seed.\n",
        "    history = model.fit(x, y, batch_size=128, epochs=1)\n",
        "    loss.append(history.history['loss'][0])\n",
        "    \n",
        "    # Create directory to store text for each epoch\n",
        "    epoch_dir = os.path.join(base_dir, 'epoch_' + str(epoch))\n",
        "    if not os.path.isdir(epoch_dir):\n",
        "        os.mkdir(epoch_dir)\n",
        "    \n",
        "    # Select a random seed text to feed into model and generate text\n",
        "    start_idx = np.random.randint(0, len(data) - maxlen - 1)\n",
        "    seed_text = data[start_idx:start_idx + maxlen]\n",
        "    for temp in [0.2, 0.5, 1.0, 1.3]:\n",
        "        generated_text = seed_text\n",
        "        temp_file = 'epoch' + str(epoch) + '_temp' + str(temp) + '.txt'\n",
        "        file = open(os.path.join(epoch_dir, temp_file), 'w')\n",
        "        file.write(generated_text)\n",
        "        \n",
        "        # Predict and generate 600 characters (approx. 1 sonnet length)\n",
        "        for i in range(600):\n",
        "            # Vectorize generated text\n",
        "            sampled = np.zeros((1, maxlen, len(chars)))\n",
        "            for j, char in enumerate(generated_text):\n",
        "                sampled[0, j, char_indices[char]] = 1.\n",
        "            \n",
        "            # Predict next character\n",
        "            preds = model.predict(sampled, verbose=0)[0]\n",
        "            pred_idx = sample(preds, temperature=temp)\n",
        "            next_char = chars[pred_idx]\n",
        "            \n",
        "            # Append predicted character to seed text\n",
        "            generated_text += next_char\n",
        "            generated_text = generated_text[1:]\n",
        "            \n",
        "            # Write to text file\n",
        "            file.write(next_char)\n",
        "        print('Temp ' + str(temp) + \" done.\")\n",
        "        file.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 124s 4ms/step - loss: 3.1190\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 2\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 3.0105\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 3\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 123s 4ms/step - loss: 3.0072\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 4\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 3.0061\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 5\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 3.0053\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 6\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 3.0046\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 7\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 3.0029\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 8\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 3.0017\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 9\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.9990\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 10\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.9966\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 11\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.9917\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 12\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.9844\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 13\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.9715\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 14\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.9491\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 15\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 123s 4ms/step - loss: 2.9080\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 16\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 123s 4ms/step - loss: 2.8618\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 17\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 123s 4ms/step - loss: 2.8247\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 18\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 123s 4ms/step - loss: 2.7916\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 19\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 123s 4ms/step - loss: 2.7623\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 20\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.7304\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 21\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.6939\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 22\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.6441\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 23\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 121s 4ms/step - loss: 2.5835\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 24\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 121s 4ms/step - loss: 2.5234\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 25\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 121s 4ms/step - loss: 2.4790\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 26\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 121s 4ms/step - loss: 2.4434\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 27\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 121s 4ms/step - loss: 2.4152\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 28\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 121s 4ms/step - loss: 2.3841\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 29\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.3613\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 30\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.3384\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 31\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.3146\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 32\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.2943\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 33\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.2751\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 34\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.2518\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 35\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.2314\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 36\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.2097\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 37\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.1880\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 38\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 122s 4ms/step - loss: 2.1682\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 39\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 121s 4ms/step - loss: 2.1464\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n",
            "Epoch 40\n",
            "Epoch 1/1\n",
            "31537/31537 [==============================] - 123s 4ms/step - loss: 2.1292\n",
            "Temp 0.2 done.\n",
            "Temp 0.5 done.\n",
            "Temp 1.0 done.\n",
            "Temp 1.3 done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgkDaxlh_PNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_sonnet(temp):\n",
        "  start_idx = np.random.randint(0,len(data)-maxlen-1)\n",
        "  new_sonnet=data[start_idx:start_idx+maxlen]\n",
        "  sys.stdout.write(new_sonnet)\n",
        "  \n",
        "  for i in range(600):\n",
        "    sampled = np.zeros((1,maxlen,len(chars)))\n",
        "    for j,char in enumerate(new_sonnet):\n",
        "      sampled[0,j,char_indices[char]]=1\n",
        "\n",
        "    preds = model.predict(sampled,verbose=0)[0]\n",
        "    pred_idx = sample(preds, temperature=temp)\n",
        "    next_char = chars[pred_idx]\n",
        "\n",
        "    new_sonnet += next_char\n",
        "    new_sonnet = new_sonnet[1:]\n",
        "    sys.stdout.write(next_char)\n",
        "    sys.stdout.flush()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6zs9nXeZAT_",
        "colab_type": "code",
        "outputId": "5c5f5f0e-8d34-470d-87ad-e3e7e44db67f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "generate_sonnet(0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " mind:\n",
            "look! what an unthrift in the worllves'ding,\n",
            "i whou so lat meserave peacist thee bides these whale\n",
            "in dade southes in thee ofneur to lose,\n",
            "the mathy theurores thouthes thou fothet beingeling,\n",
            "  fat mins so toue wath thou thige sele wertere,\n",
            "mat or me thee tine nome sereerored theres cheer weres pove,\n",
            "that thels hesen though tron soulce crace\n",
            "to the afeling thee thees on thou bethese are woree,\n",
            "whis the sore in thing four iby thee sores thee freing,\n",
            "thouch thou meoferes there thy yons thy irresthet these,\n",
            "on berires tin the is in the for thee forethen ande-me thee home,\n",
            "arane than thou tho leartes thear, and meithen\n",
            "sath the m"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWuczTtCZD5j",
        "colab_type": "code",
        "outputId": "5f307340-368a-4de8-b0fe-f4434069f73f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "generate_sonnet(0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "o! change thy thought, that i may changeres the mererereres thee more,\n",
            "where theus the the mereres the the moult,\n",
            "   and the mere the the the sore thee there,\n",
            "the sore the thee the the souther the thee there,\n",
            "whin the sore the mereres the theur thee beart,\n",
            "thou the theast the sore thou the there,\n",
            "and the the thee the in the sore the mereres the mother thee,\n",
            "the sore the sore the the sore the shathes there the meatheres and thee whoughes thee thee thee thee,\n",
            "the the sore the mereres the mase the mererest the sore,\n",
            "what the southeres thou the the sore thee thee thee theare,\n",
            "the sale the the sore the mealt the shearteres thee anderes"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghWTGcwzZJRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('Shakespeare_sonnets.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCQhaCVbZYf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}